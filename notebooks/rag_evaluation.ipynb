{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## RAG Evaluation",
   "id": "c85316079a6b3698"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Setup ",
   "id": "62915ad2cb28b543"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T19:45:04.320461Z",
     "start_time": "2024-10-28T19:45:04.311802Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "from meal_mentor import ingest"
   ],
   "id": "950ab889f97d96c7",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T19:12:16.232485Z",
     "start_time": "2024-10-28T19:12:16.164949Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load environment variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Access the environment variable\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "if OPENAI_API_KEY is not None:\n",
    "    print(\"Loaded OPENAI_API_KEY\")\n",
    "    client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "else:\n",
    "    print(\"Something went wrong; Make sure you have added the OPENAI_API_KEY in the .env file\")\n"
   ],
   "id": "f4cf851645c26113",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded OPENAI_API_KEY\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T19:12:19.298303Z",
     "start_time": "2024-10-28T19:12:19.226702Z"
    }
   },
   "cell_type": "code",
   "source": [
    "documents = ingest.ingest_data(file_path=Path(\"../data/data.csv\"))\n",
    "text_fields = [\n",
    "    \"recipe_name\",\n",
    "    \"diet_type\",\n",
    "    \"cuisine_type\",\n",
    "    \"protein(g)\",\n",
    "    \"carbs(g)\",\n",
    "    \"fat(g)\",\n",
    "]\n",
    "keyword_fields = [\"id\"]\n",
    "\n",
    "index = ingest.load_index(\n",
    "    documents=documents,\n",
    "    text_fields=text_fields,\n",
    "    keyword_fields=keyword_fields\n",
    ")"
   ],
   "id": "f6c1e5344b664045",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id diet_type                                  recipe_name cuisine_type  \\\n",
      "0   0      dash  Potato Latkes Made Simple: A Twitter Recipe       kosher   \n",
      "1   1      dash                             Avocado Dressing     american   \n",
      "2   2      dash                                  Bread Salad     american   \n",
      "3   3      dash                             Ultimat Sparkler        world   \n",
      "4   4      dash                                  Yogurt Pops     american   \n",
      "\n",
      "   protein(g)  carbs(g)  fat(g)  \n",
      "0       31.55    110.84  118.28  \n",
      "1        4.18     18.97  118.42  \n",
      "2       44.09    153.84   86.03  \n",
      "3        0.32     16.32    0.08  \n",
      "4        9.07     34.05    7.90  \n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T19:12:24.583182Z",
     "start_time": "2024-10-28T19:12:24.551339Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the ground truth dataset\n",
    "df_question = pd.read_csv('../data/ground-truth-retrieval_4o_mini.csv')\n",
    "ground_truth = df_question.to_dict(orient='records')\n",
    "record = ground_truth[0]\n",
    "record"
   ],
   "id": "ccb5ed7b11c08e7b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 0,\n",
       " 'question': 'Can you provide the nutritional breakdown for the Potato Latkes recipe, including protein, carbs, and fat content?'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T19:12:26.389539Z",
     "start_time": "2024-10-28T19:12:26.376624Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Take a sample of the ground truth dataset\n",
    "df_sample = df_question.sample(n=200, random_state=1)\n",
    "sample = df_sample.to_dict(orient='records')"
   ],
   "id": "e0c05da4da95b896",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T19:12:31.601513Z",
     "start_time": "2024-10-28T19:12:31.591859Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prompt_template = \"\"\"\n",
    "You're a healthy recipe recommender. Answer the QUESTION based on the CONTEXT from our recipe database.\n",
    "Use only the facts from the CONTEXT when answering the QUESTION. Do not ask any follow up questions.\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\"\"\".strip()\n",
    "\n",
    "entry_template = \"\"\"\n",
    "recipe_name: {recipe_name}\n",
    "diet_type: {diet_type}\n",
    "protein: {protein(g)}g\n",
    "carbs: {carbs(g)}g\n",
    "fat: {fat(g)}g\n",
    "cuisine_type: {cuisine_type}\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "def build_prompt(query, search_results):\n",
    "    context = \"\"\n",
    "\n",
    "    for doc in search_results:\n",
    "        context = context + entry_template.format(**doc) + \"\\n\\n\"\n",
    "\n",
    "    return prompt_template.format(question=query, context=context).strip()"
   ],
   "id": "edc1684871c1fc7d",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T19:12:34.312706Z",
     "start_time": "2024-10-28T19:12:34.301733Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def search(query):\n",
    "    boost = {}\n",
    "\n",
    "    results = index.search(\n",
    "        query=query,\n",
    "        filter_dict={},\n",
    "        boost_dict=boost,\n",
    "        num_results=10\n",
    "    )\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def llm(prompt, model='gpt-4o-mini'):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "def rag(query):\n",
    "    search_results = search(query)\n",
    "    prompt = build_prompt(query, search_results)\n",
    "    answer = llm(prompt)\n",
    "    return answer\n"
   ],
   "id": "5d2ecebebff23590",
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-28T19:12:36.243610Z",
     "start_time": "2024-10-28T19:12:36.237923Z"
    }
   },
   "source": [
    "rag_evaluation_prompt_template = \"\"\"\n",
    "You are an expert evaluator for a RAG system.\n",
    "Your task is to analyze the relevance of the generated answer to the given question.\n",
    "Based on the relevance of the generated answer, you will classify it\n",
    "as \"NON_RELEVANT\", \"PARTLY_RELEVANT\", or \"RELEVANT\".\n",
    "\n",
    "Here is the data for evaluation:\n",
    "\n",
    "Question: {question}\n",
    "Generated Answer: {answer_llm}\n",
    "\n",
    "Please analyze the content and context of the generated answer in relation to the question\n",
    "and provide your evaluation in parsable JSON without using code blocks:\n",
    "\n",
    "{{\n",
    "  \"Relevance\": \"NON_RELEVANT\" | \"PARTLY_RELEVANT\" | \"RELEVANT\",\n",
    "  \"Explanation\": \"[Provide a brief explanation for your evaluation]\"\n",
    "}}\n",
    "\"\"\".strip()"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Evaluation for GPT-4o-Mini",
   "id": "efadfed459cb388a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T19:42:31.293442Z",
     "start_time": "2024-10-28T19:34:04.609004Z"
    }
   },
   "cell_type": "code",
   "source": [
    "evaluations = []\n",
    "\n",
    "for record in tqdm(sample):\n",
    "    question = record['question']\n",
    "    answer_llm = rag(question)\n",
    "\n",
    "    prompt = rag_evaluation_prompt_template.format(\n",
    "        question=question,\n",
    "        answer_llm=answer_llm\n",
    "    )\n",
    "\n",
    "    evaluation = llm(prompt, model='gpt-4o-mini')\n",
    "    evaluation = json.loads(evaluation)\n",
    "\n",
    "    evaluations.append((record, answer_llm, evaluation))"
   ],
   "id": "5a07cd071c7f2e97",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3b4f6fa4feba470f89e27295306d28e7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T19:42:38.725361Z",
     "start_time": "2024-10-28T19:42:38.708033Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_eval = pd.DataFrame(evaluations, columns=['record', 'answer', 'evaluation'])\n",
    "\n",
    "df_eval['id'] = df_eval.record.apply(lambda d: d['id'])\n",
    "df_eval['question'] = df_eval.record.apply(lambda d: d['question'])\n",
    "\n",
    "df_eval['relevance'] = df_eval.evaluation.apply(lambda d: d['Relevance'])\n",
    "df_eval['explanation'] = df_eval.evaluation.apply(lambda d: d['Explanation'])"
   ],
   "id": "d011bd9030bfc11e",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T19:42:40.699683Z",
     "start_time": "2024-10-28T19:42:40.686162Z"
    }
   },
   "cell_type": "code",
   "source": "df_eval.relevance.value_counts()",
   "id": "961e6b1eb860ddfc",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "relevance\n",
       "RELEVANT           151\n",
       "PARTLY_RELEVANT     29\n",
       "NON_RELEVANT        20\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Results:\n",
    "\n",
    "| Relevance       | Count |\n",
    "|-----------------|-------|\n",
    "| RELEVANT        | 151   |\n",
    "| PARTLY_RELEVANT | 29    |\n",
    "| NON_RELEVANT    | 20    |"
   ],
   "id": "98e93ff850346a18"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T19:43:20.569891Z",
     "start_time": "2024-10-28T19:43:20.559773Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Display relevance proportions\n",
    "df_eval.relevance.value_counts(normalize=True)"
   ],
   "id": "22ca26e1aeb0c7fd",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "relevance\n",
       "RELEVANT           0.755\n",
       "PARTLY_RELEVANT    0.145\n",
       "NON_RELEVANT       0.100\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Results:\n",
    "\n",
    "| Relevance       | Proportion |\n",
    "|-----------------|------------|\n",
    "| RELEVANT        | 0.755      |\n",
    "| PARTLY_RELEVANT | 0.145      |\n",
    "| NON_RELEVANT    | 0.100      |"
   ],
   "id": "b42cab159a5b50b6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T19:43:35.934483Z",
     "start_time": "2024-10-28T19:43:35.917734Z"
    }
   },
   "cell_type": "code",
   "source": "df_eval.to_csv('../data/rag-eval-gpt-4o-mini.csv', index=False)",
   "id": "604e4bd1d1d68937",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Evaluation for GPT-4o",
   "id": "4ecfea7d7e3f19b5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T19:31:30.751018Z",
     "start_time": "2024-10-28T19:21:51.263389Z"
    }
   },
   "cell_type": "code",
   "source": [
    "evaluations = []\n",
    "\n",
    "for record in tqdm(sample):\n",
    "    question = record['question']\n",
    "    answer_llm = rag(question)\n",
    "\n",
    "    prompt = rag_evaluation_prompt_template.format(\n",
    "        question=question,\n",
    "        answer_llm=answer_llm\n",
    "    )\n",
    "\n",
    "    evaluation = llm(prompt, model='gpt-4o')\n",
    "    evaluation = json.loads(evaluation)\n",
    "\n",
    "    evaluations.append((record, answer_llm, evaluation))"
   ],
   "id": "3ef039e95657715a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e5fbfb7095b04f6a87fda7176b3714e2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T19:32:28.130323Z",
     "start_time": "2024-10-28T19:32:28.113117Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_eval = pd.DataFrame(evaluations, columns=['record', 'answer', 'evaluation'])\n",
    "\n",
    "df_eval['id'] = df_eval.record.apply(lambda d: d['id'])\n",
    "df_eval['question'] = df_eval.record.apply(lambda d: d['question'])\n",
    "\n",
    "df_eval['relevance'] = df_eval.evaluation.apply(lambda d: d['Relevance'])\n",
    "df_eval['explanation'] = df_eval.evaluation.apply(lambda d: d['Explanation'])"
   ],
   "id": "c9f2fce6f69cb29b",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T19:32:40.811009Z",
     "start_time": "2024-10-28T19:32:40.796628Z"
    }
   },
   "cell_type": "code",
   "source": "df_eval.relevance.value_counts()",
   "id": "bb9de090a56f5f5b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "relevance\n",
       "RELEVANT           95\n",
       "PARTLY_RELEVANT    76\n",
       "NON_RELEVANT       29\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Results:\n",
    "\n",
    "| Relevance       | Count |\n",
    "|-----------------|-------|\n",
    "| RELEVANT        | 95    |\n",
    "| PARTLY_RELEVANT | 76    |\n",
    "| NON_RELEVANT    | 29    |"
   ],
   "id": "eb53189de6ff8121"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T19:33:26.274056Z",
     "start_time": "2024-10-28T19:33:26.259304Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Display relevance proportions\n",
    "df_eval.relevance.value_counts(normalize=True)"
   ],
   "id": "1a0fc61bb28869ce",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "relevance\n",
       "RELEVANT           0.475\n",
       "PARTLY_RELEVANT    0.380\n",
       "NON_RELEVANT       0.145\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Results:\n",
    "\n",
    "| Relevance       | Proportion |\n",
    "|-----------------|------------|\n",
    "| RELEVANT        | 0.475      |\n",
    "| PARTLY_RELEVANT | 0.380      |\n",
    "| NON_RELEVANT    | 0.145      |"
   ],
   "id": "5a6ffc4efdd31d1c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T19:33:53.855739Z",
     "start_time": "2024-10-28T19:33:53.842648Z"
    }
   },
   "cell_type": "code",
   "source": "df_eval.to_csv('../data/rag-eval-gpt-4o.csv', index=False)",
   "id": "3aea54eaaec3ca5e",
   "outputs": [],
   "execution_count": 19
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
